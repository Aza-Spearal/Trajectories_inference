{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "[train_dataset, valloader] = torch.load('wikiloader_ssquote_ssbord')\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "device='cuda'\n",
    "sent = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, d_emb, original_model):\n",
    "        super().__init__()\n",
    "        hidden_size = original_model.config.hidden_size\n",
    "        \n",
    "        self.emb = nn.Linear(d_emb, hidden_size)\n",
    "        self.emb_rev = nn.Linear(hidden_size, d_emb)\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1\n",
    "        )\n",
    "        self.mod = get_peft_model(original_model, lora_config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        x = self.mod(inputs_embeds=x)\n",
    "        x = x.hidden_states[-1]\n",
    "        \n",
    "        x = self.emb_rev(x)\n",
    "        return x\n",
    "\n",
    "#orig_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", output_hidden_states=True)\n",
    "#orig_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "orig_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(sent.get_sentence_embedding_dimension(), orig_model).to(device)\n",
    "\n",
    "\n",
    "optimizer = Adam([\n",
    "    *model.emb.parameters(),\n",
    "    *model.emb_rev.parameters(),\n",
    "    *model.mod.parameters(),\n",
    "])\n",
    "\n",
    "pad = 99\n",
    "\n",
    "def criterion(output, target):\n",
    "    mask = target == pad\n",
    "    output = output[~mask]\n",
    "    target = target[~mask]\n",
    "    mse = nn.MSELoss(reduction='none'); out = mse(output, target).sum()\n",
    "    #mse = nn.MSELoss(); out = mse(output, target)\n",
    "    #cossim = nn.CosineSimilarity(dim=-1, eps=1e-6); out = 1-cossim(output, target) #PEUT-ETRE DIM=1\n",
    "    return out\n",
    "\n",
    "val_list=[]\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = next(iter(valloader))\n",
    "        outputs = model(src[:, :-1])\n",
    "        val_loss = criterion(outputs, src[:, 1:]).item()\n",
    "        print('Epoch', str(epoch+1) + ', Loss:', val_loss)\n",
    "        val_list.append(val_loss)\n",
    "\n",
    "test(-1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    for src in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src[:, :-1])\n",
    "        loss = criterion(outputs, src[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "    test(epoch)\n",
    "    \n",
    "    #early stopping code\n",
    "    if val_list[-1] < min(val_list[:-1]):#its performs better, we save the model\n",
    "        model_save = model\n",
    "    elif (len(val_list) - val_list.index(min(val_list))) > 3: #no better model in the last epochs\n",
    "        break;\n",
    "\n",
    "model = model_save\n",
    "plt.plot(val_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict(transfo, src, steps=max_len):\n",
    "    if steps > max_len:\n",
    "        print('steps could not be superior to max_len (who is '+str(max_len)+')')\n",
    "        print('steps is set to '+str(max_len))\n",
    "        steps = max_len\n",
    "    if steps == 0:\n",
    "        print('steps is set to 1')\n",
    "        steps = 1\n",
    "\n",
    "    transfo.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        text = sent.encode(src, convert_to_tensor=True).to(\"cuda\").unsqueeze(0)\n",
    "        \n",
    "        while text.size(1) < steps:\n",
    "            output = (transfo(text))[:,-1:]\n",
    "            text = torch.cat((text, output), dim=1)\n",
    "\n",
    "        similarities = util.semantic_search(text[0, 1:], corpus_embedding) #delete the batch and the first token\n",
    "        affichage = [[f\"{round(sim['score'], 2)} {corpus[sim['corpus_id']]}\" for sim in liste] for liste in similarities] #score, corpus dans un string\n",
    "\n",
    "\n",
    "        for aff in src:\n",
    "            print(aff, '\\n')\n",
    "        for aff in affichage:\n",
    "            print(aff[0])\n",
    "        \n",
    "        df = pd.DataFrame(affichage).transpose()\n",
    "        df.columns = [i+len(src) for i in df.columns]\n",
    "        \n",
    "        for i, source in enumerate(src):\n",
    "            new_column = [source] + [\"\"] * (len(df) - 1)\n",
    "            df.insert(i, str(i), new_column)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "#src = [' = John Lenon = \\n']\n",
    "#src = ['John Lenon']\n",
    "src = ['= John Lenon =', 'John Winston Ono Lennon(born John Winston Lennon; 9 October 1940 - 8 December 1980) was an English singer-songwriter, musician and political activist. He gained worldwide fame as the founder, co-lead vocalist and rhythm guitarist of the Beatles. His work included music, writing, drawings and film. His songwriting partnership with Paul McCartney remains the most successful in history as the primary songwriters in the Beatles.']\n",
    "#src = ['Chocolate cake recipe']\n",
    "#src = ['= Chocolate cake =', 'Chocolate cake or chocolate gâteau (from French: gâteau au chocolat) is a cake flavored with melted chocolate, cocoa powder, or both. It can also have other ingredients such as fudge, vanilla creme, and other sweeteners.']\n",
    "#src = ['import pandas as pd']\n",
    "predict(model, src)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
